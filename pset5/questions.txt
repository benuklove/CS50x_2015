0.  A lung disease otherwise known as silicosis, the longest word publihed in the OED, and the longest word to be loaded from our dictionary file.
1.  getrusage returns resource usage measures for the calling process, which is the sum of resources used by  all threads in the process.
2.  16 member usages
3.  The reason we pass before and after by reference instead of by value has to be because accessing the contents of a pointer is quicker than accessing the value of before or after.
4.  The for loop iterates over each character in the text until EOF, and updates the loop with the next char.  As it iterates, if it's alpha or apostrophe, it appends that character to a character array, word, provided it is no longer than the max LENGTH (if so, the while loop ends the word and sets the index to zero for a new word).  Skipping over numbers, if the word is finished (index is greater than zero and word [index] is not alpha or apostrophe), the last else if statement terminates the word, updates words, checks if it is in the dictionary, updates misspellings if not, updates the time it took to do this, and sets index back to zero for a new word. 
5.  With fscanf, any nonwhitespace character is valid, so any text that includes numbers or other nonalpha characters would be included.  This would cause possible misspellings when misspellings are not actually happening.
6.  check and load parameters can be specified constant and put in read only memory to optimize the program's runtime.
7.  I used a trie data structure.  With great assistance from Zamyla's walkthrough, it has a root node with 27 cells in the array (each letter plus apostrophe), each cell of each node has two components: a boolean conditional to say whether it's the end of a word, and a pointer to the next character's node if one is needed.  
8.  It was so slow it would just get "killed" - I assume by the appliance.  It's much faster now, but I could never effectively use valgrind to eliminate *all* leaks.  I know I'll be penalized for it.
9.  I was allocating 27 elements for every single node, which was okay with a small dictionary.  But for a large dictionary it bogged down big time.
10. From what I've read, it seems like calloc may use more resources than malloc?  I got calloc working better than I ever did with malloc.  I also hate that I ran every character through my lowercase function in 'check'.  I should have found a way to speed that up.
